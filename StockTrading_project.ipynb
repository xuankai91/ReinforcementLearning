{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StockTrading_project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3QMS9OAGiuhz"
      ],
      "authorship_tag": "ABX9TyOwYamB3vL0UAZpcKi7uQ/w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xuankai91/ReinforcementLearning/blob/main/StockTrading_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLaNOAcfvBh2"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from datetime import datetime\n",
        "import itertools\n",
        "import argparse\n",
        "import os\n",
        "import re\n",
        "import pickle"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eW8NloqZvPbl"
      },
      "source": [
        "# Environment overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaMoF0u6vSpJ"
      },
      "source": [
        "For this project, we focus on the trading of three stocks: Apple Inc. (AAPL), Motorola, Inc. (MSI), & Starbucks Corporation (SBUX).\n",
        "\n",
        "---\n",
        "**State**\n",
        "\n",
        "the state vector contains\n",
        " 1. How many shares of each owned stock\n",
        " 2. Current price of each stock\n",
        " 3. How much uninvested cash available\n",
        "\n",
        "therefore, $\\mathcal{S} \\in \\Bbb{R}^{7}$\n",
        "\n",
        "---\n",
        "**Action**\n",
        "\n",
        "Based on the All-or-nothing approach. For each share, we can\n",
        " 1. Buy - buy as many shares as possible (round robin, buy 1 share per round)\n",
        " 2. Sell - sell all shares of the stock that is owned\n",
        " 3. Hodl\n",
        "\n",
        "thus $\\mathcal{A} \\in \\Bbb{R}^{27}$\n",
        "\n",
        "We assume no transaction costs (i.e. spread). Furthermore, we specify the order of actions such that our agent will sell stocks we want to sell before we buy any more. \n",
        "\n",
        "---\n",
        "**Reward**\n",
        "\n",
        "Change in the value of portfolio from one step ($s$) to the next ($s'$), and is simply calculated\n",
        "\n",
        "$$\\text{portfolio value} = \\text{number of shares} * \\text{price} + \\text{uninvested cash}$$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kF_S53Y-uhvh"
      },
      "source": [
        "class StockEnv:\n",
        "  def __init__(self,data,initial_investment = 20000):\n",
        "    # data\n",
        "    '''\n",
        "    this is the historical price data of three stocks - AAPL, MSI, SBUX - at daily closing\n",
        "    '''\n",
        "    self.stock_price_history = data\n",
        "    self.n_step, self.n_stock = self.stock_price_history.shape\n",
        "\n",
        "    # instance attributes\n",
        "    self.initial_investment = initial_investment\n",
        "    self.curr_step = None\n",
        "    \n",
        "    # state space\n",
        "    '''\n",
        "    state is a 7-tuple consisting of the following entries, in order:\n",
        "     - quantities of each of the three stocks owned (3 entries)\n",
        "     - price of each of the three stocks owned (3 entries)\n",
        "     - cash owned (1 entry)\n",
        "    '''\n",
        "    self.stock_owned = None\n",
        "    self.stock_price = None\n",
        "    self.cash_in_hand = None\n",
        "\n",
        "    self.state_dim = 2*self.n_stock + 1\n",
        "\n",
        "    # action space\n",
        "    '''\n",
        "    for each stock, the actions that can be performed is:\n",
        "      0 = sell\n",
        "      1 = hodl\n",
        "      2 = buy\n",
        "    the permutations will then be an action list, such that, for e.g.\n",
        "     - [0,0,0]: sell AAPL, sell MSI, sell SBUX\n",
        "     - [2,0,1]: buy AAPL, sell MSI, hodl SBUX\n",
        "    '''\n",
        "    self.action_space = np.arange(3**self.n_stock)\n",
        "    self.action_list = list(map(list,itertools.product([0,1,2],repeat = self.n_stock)))\n",
        "    \n",
        "    # reset\n",
        "    self.reset()\n",
        "   \n",
        "  ## --------------------------------------------------------------------- ##\n",
        "  # the design of this environment conforms to the format by OpenAI gym\n",
        "\n",
        "  def reset(self):\n",
        "    self.curr_step = 0\n",
        "    self.stock_owned = np.zeros(self.n_stock)\n",
        "    self.stock_price = self.stock_price_history[self.curr_step] #by rows\n",
        "    self.cash_in_hand = self.initial_investment\n",
        "    return self._get_obs()\n",
        "\n",
        "  def step(self,action):\n",
        "    assert action in self.action_space\n",
        "\n",
        "    # get current Qvalue before performing the action\n",
        "    prev_val = self._get_val()\n",
        "\n",
        "    # update price, i.e. go to next day\n",
        "    self.curr_step += 1\n",
        "    self.stock_price = self.stock_price_history[self.curr_step]\n",
        "\n",
        "    # perform the trade\n",
        "    self._trade(action)\n",
        "\n",
        "    # get new value after taking the action\n",
        "    curr_val = self._get_val()\n",
        "\n",
        "    # reward(increase in portfolio value)\n",
        "    reward = curr_val - prev_val \n",
        "    \n",
        "    # if we have ran out of data, terminate episode\n",
        "    done = self.curr_step == self.n_step-1 #return True if at the last step, False otherwise\n",
        "\n",
        "    # store info of current portfolio value\n",
        "    info = {'current_value':curr_val}\n",
        "\n",
        "    # END OF FUNCTION, returns: new_state, reward, done_flag, info\n",
        "    return self._get_obs(), reward, done, info\n",
        "\n",
        "  ## --------------------------------------------------------------------- ##\n",
        "  \n",
        "  def _get_obs(self):\n",
        "    obs = np.empty(self.state_dim)\n",
        "    obs[:self.n_stock] = self.stock_owned\n",
        "    obs[self.n_stock:self.n_stock*2] = self.stock_owned.dot(self.stock_price)\n",
        "    obs[-1] = self.cash_in_hand\n",
        "    return obs\n",
        "\n",
        "  def _get_val(self):\n",
        "    return self.stock_owned.dot(self.stock_price) + self.cash_in_hand\n",
        "\n",
        "  def _trade(self,action):\n",
        "    '''\n",
        "    again, the following keys are:\n",
        "      0: sell\n",
        "      1: hodl\n",
        "      2: buy\n",
        "    we define our trade sequence (for each stock) as follows:\n",
        "      1. sell all of the stocks we want to sell\n",
        "      2. then buy more stocks that we want to buy\n",
        "      3. if hodl, do not do anything\n",
        "    '''\n",
        "    action_vec = self.action_list[action] #action should be an integer, and returns a vector of size 3\n",
        "\n",
        "    # determine which stocks to buy or sell\n",
        "    sell_index = []\n",
        "    buy_index = []\n",
        "    for i,a in enumerate(action_vec):\n",
        "      if a == 0:\n",
        "        sell_index.append(i)\n",
        "      elif a == 2:\n",
        "        buy_index.append(i)\n",
        "\n",
        "    # first, sell stocks we want to sell\n",
        "    if sell_index: #if there are stocks to sell\n",
        "      for idx in sell_index:\n",
        "        self.cash_in_hand += self.stock_owned[idx] * self.stock_price[idx]\n",
        "        self.stock_owned[idx] = 0\n",
        "\n",
        "    # second, we buy stocks that we want to buy\n",
        "    if buy_index: #if there are stocks to buy\n",
        "      can_buy = True\n",
        "      while can_buy: # we loop through each stock we want to buy, and buy 1 share at a time\n",
        "        for idx in buy_index:\n",
        "          if self.cash_in_hand < self.stock_price[idx]: #if can't afford to buy anymore\n",
        "            can_buy = False\n",
        "          else:\n",
        "            self.stock_owned[idx] += 1 #buy 1 share\n",
        "            self.cash_in_hand -= self.stock_price[idx] #deduct price\n",
        "\n",
        "## ------------------------------------------------------------------"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKHYWQ77vSNS"
      },
      "source": [
        "# Agent description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIaSAXP_6ZRW"
      },
      "source": [
        "---\n",
        "**Function Approximation**\n",
        "\n",
        "* We implement a linear function, analogous to a single layer neural network, with Q-learning to train our RL agent.\n",
        "\n",
        "* As part of learning, we estimate our Q-value as follows:\n",
        "$$\\underbrace{Q(s,\\mathcal{A})}_{|\\mathcal{A}| \\times 1} = \\underbrace{W^{T}}_{|\\mathcal{A}| \\times |\\mathcal{S}|} \\cdot \\underbrace{s}_{|\\mathcal{S}| \\times 1} + \\underbrace{b}_{|\\mathcal{A}| \\times 1}$$\n",
        "which essentially means that we will approximate Q-values for all actions as a vector.\n",
        "\n",
        "* However, as we select only 1 action to execute our next step, we first recognise that the target for Q-learning, $G$ is a scalar.\n",
        "$$G =\n",
        "\\begin{cases}\n",
        "r & \\text{if} \\ s' = s_{T}\n",
        "\\\\ r + \\gamma \\max_{a'}Q(s',a') & \\text{otherwise}\n",
        "\\end{cases}$$\n",
        "\n",
        "* second, the cost function of squared error, $J = (G - W^{T}s)^{2}$, has the gradient\n",
        "$$\\nabla_{w}J = \\frac{2}{N} (G - W^{T}s)s $$\n",
        "\n",
        "* to assist in the implementation of gradient ascent, we expand $G$ into a vector, $G \\in \\Bbb{R}^{|\\mathcal{A}|}$. Thus, if we select action $a_{k}$, we define each element of $G$, indexed by $i$, as such\n",
        "$$g_{i} =\n",
        "\\begin{cases}\n",
        "r + \\gamma \\underbrace{\\max_{a'}Q(s',a')}_{0 \\ \\text{if} \\ s' = s_{T}} & \\text{if} \\ i = k\n",
        "\\\\ Q(s,a) & \\text{otherwise}\n",
        "\\end{cases}$$\n",
        "This ensures that the gradient will be 0 for unselected actions, and thus will not change the weights matrix $W$ at every update except for selected actions.\n",
        "\n",
        "--- \n",
        "**Momentum for SGA**\n",
        "\n",
        "* to speed up convergence of weight updating, we also implment momentum in the form:\n",
        "$$v(t) = \\mu \\cdot v(t-1) - \\eta \\cdot g(t)$$\n",
        "where:\n",
        " * $g(t)$ is the gradient at iteration/step $t$\n",
        " * $\\mu$ is a hyperparameter indicating momentum, recommended to be $\\in [0.9,1]$\n",
        " * $\\eta$ is learning rate\n",
        "* the weights at time $t$ is then updated by\n",
        "$$w(t) \\leftarrow w(t-1) + v(t)$$\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB7stZD2e4Nc"
      },
      "source": [
        "# FUNCTION APPROX MODEL\n",
        "class LinearModel:\n",
        "  def __init__(self,input_size,output_size):\n",
        "    '''\n",
        "    This is in the style of a single-layer neural network\n",
        "    input_size should correspond with the length of the state vector\n",
        "    output_size should correspond with the length of the action space\n",
        "    '''\n",
        "\n",
        "    # model weights\n",
        "    self.W = np.random.randn(input_size,output_size)/np.sqrt(input_size)\n",
        "    self.b = np.zeros(output_size)\n",
        "\n",
        "    # momentum\n",
        "    self.vW = 0\n",
        "    self.vb = 0\n",
        "\n",
        "    # losses\n",
        "    self.losses = []\n",
        "\n",
        "  def predict(self,X):\n",
        "    # make sure X is NxD\n",
        "    assert(len(X.shape)==2) #X must at least be a column vector\n",
        "    return X.dot(self.W) + self.b\n",
        "\n",
        "  def gradient_descent(self,X,Y,learning_rate = .01, momentum = .9):\n",
        "    '''\n",
        "    performs gradient descent on the dataset\n",
        "    X should be matrix of states\n",
        "    Y should be the target, r + gamma*Q(s',a') (or just r if s'==0)\n",
        "    '''\n",
        "\n",
        "    # make sure X is NxD\n",
        "    assert(len(X.shape)==2)\n",
        "\n",
        "    # get number of outputs\n",
        "    num_values = np.prod(Y.shape)\n",
        "\n",
        "    # update weights from Q-values\n",
        "    Yhat = self.predict(X) #this is Q(s,a)\n",
        "    gW = 2/num_values * X.T.dot(Yhat-Y) #Y is the target, provided as input\n",
        "    gb = 2/num_values * (Yhat-Y).sum(axis=0) \n",
        "\n",
        "    # update momentum terms\n",
        "    self.vW = momentum * self.vW - learning_rate * gW\n",
        "    self.vb = momentum * self.vb - learning_rate * gb\n",
        "\n",
        "    # update weights\n",
        "    self.W += self.vW\n",
        "    self.b = self.vb\n",
        "\n",
        "    #calculate mse\n",
        "    mse = np.mean((Yhat-Y)**2)\n",
        "    self.losses.append(mse)\n",
        "\n",
        "\n",
        "    def load_weights(self,filename):\n",
        "      npz = np.load(filename)\n",
        "      self.W = npz['W']\n",
        "      self.b = npz['b']\n",
        "\n",
        "    def save_weights(self,filename):\n",
        "      np.savez(filename, W = self.W, b = self.b)\n",
        "\n",
        "\n",
        "## ------------------------------------------------------------------\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KpANvftR98E"
      },
      "source": [
        "class DQNAgent():\n",
        "  def __init__(self,state_size,action_size,GAMMA,ALPHA,ETA):\n",
        "    '''\n",
        "    This is the Agent\n",
        "    state_size is the length of the state vector\n",
        "    action_size is the length of the action space\n",
        "    '''\n",
        "\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "\n",
        "    # hyperparameters\n",
        "    self.gamma = GAMMA\n",
        "    self.alpha = ALPHA\n",
        "    self.eta = ETA\n",
        "\n",
        "    # policy epsilon; we move from more exploratory to more greedy\n",
        "    self.epsilon = 1.0 \n",
        "    self.epsilon_min = .01\n",
        "    self.epsilon_decay = .995\n",
        "\n",
        "    # define DQN model\n",
        "    self.model = LinearModel(state_size,action_size)\n",
        "  \n",
        "  def get_action(self,state):\n",
        "    if np.random.rand() <= self.epsilon:\n",
        "      return np.random.choice(self.action_size)\n",
        "    else:\n",
        "      action_values = self.model.predict(state)\n",
        "      return np.argmax(action_values.ravel())\n",
        "\n",
        "  def train(self,state,action,reward,next_state,done):\n",
        "    # get estimted returns at terminal state\n",
        "    if done:\n",
        "      target = reward\n",
        "    else:\n",
        "      target = reward + self.gamma * np.amax(self.model.predict(next_state), axis = 1)\n",
        "\n",
        "    # generate vector of estimated returns; update only the entry that corresponds with the chosen action\n",
        "    target_full = self.model.predict(state)\n",
        "    target_full[0,action] = target\n",
        "\n",
        "    # perform one step of gradient descent\n",
        "    self.model.gradient_descent(state, target_full, learning_rate = self.alpha, momentum = self.eta)\n",
        "\n",
        "    # reduce exploration rate\n",
        "    if self.epsilon > self.epsilon_min:\n",
        "      self.epsilon *= self.epsilon_decay\n",
        "\n",
        "  def load(self, filename):\n",
        "    self.model.load_weights(filename)\n",
        "\n",
        "  def save(self, filename):\n",
        "    self.model.save_weights(filename)\n",
        "\n",
        "## ------------------------------------------------------------------\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zl-M4-YST9dK"
      },
      "source": [
        "# HELPER FUNCTIONS\n",
        "\n",
        "#\n",
        "def get_scaler(env):\n",
        "  '''\n",
        "  this function plays one episode of the environment to get a distribution of state spaces\n",
        "  this allows for future state spaces to be normalised\n",
        "  returns a fitted StandardScaler instance\n",
        "  '''\n",
        "  # get some samples of states so that dataset can be scaled\n",
        "  states = []\n",
        "  for _ in range(env.n_step):\n",
        "    action = np.random.choice(env.action_space)\n",
        "    state, _,done,_ = env.step(action)\n",
        "    states.append(state)\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "  scl = StandardScaler()\n",
        "  scl.fit(states)\n",
        "  return scl\n",
        "\n",
        "#\n",
        "def play_one_episode(agent,env,scaler,is_train):\n",
        "  '''\n",
        "  this function makes the agent play 1 episode until termination\n",
        "  if it is a training instance, set is_train = 'train' to make it \"training mode\"\n",
        "  returns the portfolio value\n",
        "  '''\n",
        "  state = env.reset() #starting state\n",
        "  state = scaler.transform([state]) #normalise\n",
        "\n",
        "  # start episode\n",
        "  done = False\n",
        "  while not done:\n",
        "    action = agent.get_action(state)\n",
        "    next_state, reward, done, info = env.step(action) #obtain s', r, if has reached terminal state, misc. info\n",
        "    next_state = scaler.transform([next_state])\n",
        "    \n",
        "    # only if in training mode\n",
        "    if is_train == 'train':\n",
        "      agent.train(state,action,reward,next_state,done)\n",
        "\n",
        "    # update state\n",
        "    state = next_state\n",
        "\n",
        "  # return value of portfolio\n",
        "  return info['current_value']\n",
        "  "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss9umN3kDbhL"
      },
      "source": [
        "# TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaWUHNRni6mT"
      },
      "source": [
        "# set up training and testing dataset\n",
        "# we use half for training and half for testing\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/aapl_msi_sbux.csv').values #transform to numpy array\n",
        "n_timesteps, n_stocks = data.shape\n",
        "\n",
        "n_train = n_timesteps//2 #get midpoint of the timesteps\n",
        "\n",
        "train_data = data[:n_train] #use 1st half as training data\n",
        "test_data = data[n_train:] #use 2nd half as testing data"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgvRixLtjf8O"
      },
      "source": [
        "#setup\n",
        "n_episodes = 2000\n",
        "batch_size = 32\n",
        "initial_investment = 20000\n",
        "GAMMA = .9\n",
        "ALPHA = .05\n",
        "ETA = .95\n",
        "\n",
        "# GAMMA = .9, ALPHA = .05, ETA = .95 seems the best"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "bgxQvV3dDiLj",
        "outputId": "524dc82d-dd85-45a7-8fed-d0ff65973db5"
      },
      "source": [
        "## TRAINING AGENT\n",
        "# instantiate environment, get necessary info\n",
        "train_env = StockEnv(train_data,initial_investment)\n",
        "state_size = train_env.state_dim\n",
        "action_size = len(train_env.action_space)\n",
        "\n",
        "# instantiate agent\n",
        "agent = DQNAgent(state_size,action_size,GAMMA,ALPHA,ETA)\n",
        "\n",
        "# get scaler (for future normalisation)\n",
        "scaler = get_scaler(train_env)\n",
        "\n",
        "# record final value of portfolio (at end of episode)\n",
        "training_portfolio_value = []\n",
        "\n",
        "# play the game multiple times\n",
        "for e in range(n_episodes):\n",
        "  t0 = datetime.now() #start time\n",
        "  \n",
        "  # get portfolio value after playing 1 episode\n",
        "  val = play_one_episode(agent, train_env, scaler, 'train')\n",
        "\n",
        "  dt = datetime.now() - t0 #duration elapsed from start time\n",
        "\n",
        "  if (e+1) % 100 == 0:\n",
        "    print('episode %d of %d, portfolio value: %.2f, duration: %.3f' % (e+1,n_episodes,val,dt.seconds+dt.microseconds*1e-6))\n",
        "  training_portfolio_value.append(val)\n",
        "\n",
        "# plot losses\n",
        "print('\\n')\n",
        "_ = plt.plot(agent.model.losses)\n",
        "_ = plt.title('MSE over time')\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode 100 of 2000, portfolio value: 39635.03, duration: 0.181\n",
            "episode 200 of 2000, portfolio value: 39541.59, duration: 0.244\n",
            "episode 300 of 2000, portfolio value: 34908.08, duration: 0.137\n",
            "episode 400 of 2000, portfolio value: 32830.24, duration: 0.337\n",
            "episode 500 of 2000, portfolio value: 37992.60, duration: 0.164\n",
            "episode 600 of 2000, portfolio value: 27122.15, duration: 0.208\n",
            "episode 700 of 2000, portfolio value: 34228.10, duration: 0.272\n",
            "episode 800 of 2000, portfolio value: 20890.64, duration: 0.255\n",
            "episode 900 of 2000, portfolio value: 30138.37, duration: 0.194\n",
            "episode 1000 of 2000, portfolio value: 33385.62, duration: 0.279\n",
            "episode 1100 of 2000, portfolio value: 25571.52, duration: 0.175\n",
            "episode 1200 of 2000, portfolio value: 24189.94, duration: 0.170\n",
            "episode 1300 of 2000, portfolio value: 19216.83, duration: 0.228\n",
            "episode 1400 of 2000, portfolio value: 25603.86, duration: 0.204\n",
            "episode 1500 of 2000, portfolio value: 52174.63, duration: 0.172\n",
            "episode 1600 of 2000, portfolio value: 20888.16, duration: 0.193\n",
            "episode 1700 of 2000, portfolio value: 25721.01, duration: 0.242\n",
            "episode 1800 of 2000, portfolio value: 31640.62, duration: 0.164\n",
            "episode 1900 of 2000, portfolio value: 27121.39, duration: 0.174\n",
            "episode 2000 of 2000, portfolio value: 24919.83, duration: 0.176\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEVCAYAAADJrK/3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZgcZbn+8e9DAgKCojB4kMURBQQjmyOCcFRAdg74E1BAVI78jKAHRY9LOKiIGyIcxIUt7LKFLSASQtgSsodMFrKQfd9nsu+ZzMxz/qjqmZ6enunqmenud6bvz3XlynR3dfdT3dV3vfXWW1Xm7oiISLh2KXUBIiLSPgW1iEjgFNQiIoFTUIuIBE5BLSISOAW1iEjgFNQinWRmXzezV0tdh/RcCmrpNDNbaGZ1ZrZfxv2TzMzNrDK+fZCZPWdmq81sg5lNM7Mr48cq42k3Z/z7WtFnqB1pdfZO3efuj7v7maWsS3q23rknEUlkAXAZ8DcAM/sUsGfGNI8C7wAfAXYAnwL+LWOafdy9vrClJmNmvUOpRcqbWtTSVR4Fvpl2+1vAPzKm+QzwsLtvcfd6d5/k7oM78mZm9mEze9HM1prZXDP7Ttr928zsg2nTHhe34neNb3/bzGaY2TozG2JmH0mb1s3s+2Y2B5iT5a2Hx/+vj1v8J5nZlWY2MuM1vmdmc8xsk5n91sw+ZmajzWyjmT1tZrulTX++mU02s/XxNEd35DORnktBLV1lLPA+MzvSzHoBlwKPZZnmTjO71MwO6eT7DQCWAh8GLgb+YGanuftyYAxwUdq0lwPPuvtOM7sQ+B/gK0AFMAJ4MuO1vwx8Fjgqy/t+Pv5/H3ffy93HtFHfWcCngROBnwH9gSuAg4E+RFsfmNlxwIPAd4F9gXuBF83sPUk+BCkPBQtqM3vQzGrMbFrC6b9qZu+a2XQze6JQdUlBpVrVZwAzgGUZj19CFIy/BBbErcjPZEyzOm5Zpv4dmfkmZnYwcDLwc3ff7u6TgftpbtE/QXMQGtFKI7VMXQ3c7O4z4m6NPwDHpreq48fXuvu2jnwIsT+5+0Z3nw5MA1519/nuvgEYDBwXT9cXuNfdx7l7g7s/QtQtdGIn3lt6mEK2qB8Gzk4yoZkdBlwPnOzunwSuK2BdUjiPErVer6R1twfuvs7d+8Xf8YeAycALcZim7Ofu+6T9m5HlfT4MrHX3TWn3LQIOjP9+DjjJzA4gagE3Eq0gIOof/0tqRQCsBSztuQBL8prr7Fal/b0ty+290ur57/SVE1Gr+8NdUIP0EAULancfTvQjaBL3071iZhPMbISZfSJ+6DvAne6+Ln5uTaHqksJx90VEOxXPBQbmmHY1cBtRIH2wvWmzWA580Mz2TrvvEOIWfLwcvQp8jWjFMcCbTxO5BPhuxspgD3cfnV5ee6XnWWsuS4DfZ9Szp7tndsdIGSt2H3V/4Fp3/zTwE+Cu+P7DgcPNbJSZjTWzRC1xCdJVwGnuviXzATO7xcz6mFnvOGSvAea6+5p83sDdlwCjgZvNbPd459tVtOwTf4KoK+Rimrs9AO4BrjezT8Y1vd/MLsnj7WuJWuiH5lNzO+4Drjazz1rkvWZ2XsZKSMpc0YbnmdlewOeAZ9K2dFM7THoDhwFfBA4ChpvZp9x9fbHqk67h7vPaeXhP4HngAKLN/3HABRnTrG/ZE8Kv3P32LK91GVHoLgfWATe6++tpj79I1G+92N3fSavv+XhZHBD3S28AXgOeSTB7uPtWM/s9MCoeRdKpRoW7V8cjVv5O9BvYBoykeXSJCFbICwfEBzq85O59zOx9wCx3PyDLdPcA49z9ofj2G0A/dx9fsOJERLqJonV9uPtGoj39l0C0N97MjokffoGoNY1FR7cdDswvVm0iIiEr5PC8J4nGsx5hZkvN7Crg68BVZvYOMB24MJ58CLDGzN4FhgI/zbffUkSkpypo14eIiHSejkwUEQlcQUZ97Lfffl5ZWVmIlxYR6ZEmTJiw2t0rsj1WkKCurKykurq6EC8tItIjmdmith5T14eISOAU1CIigVNQi4gETkEtIhI4BbWISOASBbWZ/Sg+of80M3vSzHYvdGEiIhLJGdRmdiDwA6DK3fsAqcssiYhIESTt+ugN7GFmvYlOVbm8cCVF6uobeaZ6CTrEXUTKXc6gdvdlRFfiWAysADa4+6uZ05lZXzOrNrPq2traThf21zfm8NNnpzBo6opOv5aISHeWpOvjA0Rnufso0WWT3mtmV2RO5+793b3K3asqKrIeBZmXNVt2ALBxW32nX0tEpDtL0vXxJWCBu9e6+06ia+F9rrBliYhISpKgXgycaGZ7xleLPh3IdmVoEREpgCR91OOAZ4GJwNT4Of0LXJeIiMQSnT3P3W8EbixwLSIikoWOTBQRCZyCWkQkcMEHtaMDXkSkvAUc1FbqAkREghBwUIuICCioRUSCp6AWEQmcglpEJHAKahGRwCmoRUQCF2xQWzw6T9cNEJFyF25Qx/8rp0Wk3AUb1CLlbsnardQ3NJa6DAmAglokQMvXb+Pf/zSUW4fMKnUpEgAFtUiA1myuA2DUvNUlrkRCoKAWEQlckovbHmFmk9P+bTSz64pRnIiIJLjCi7vPAo4FMLNewDLg+QLXlV5A0d5KRCRE+XZ9nA7Mc/dFhSgmnekspyIiQP5BfSnwZLYHzKyvmVWbWXVtbW3nKxMRESCPoDaz3YALgGeyPe7u/d29yt2rKioquqo+EZGyl0+L+hxgoruvKlQxIiLSWj5BfRltdHuIiEjhJApqM3svcAYwsLDliIhIppzD8wDcfQuwb4FrEZGY63Rkkib4IxO1uEo5MzROVQIOai2gIiKRYIM6RQcmiki5CzaodWSiiEgk2KAWEZGIglpEJHAKahGRwCmoRUQCF3xQu4Z9iEiZCzaoNehDREK2YPUWBk1ZUZT3SnQIuYgUlzYkw3fqbcMAOO/o8wr+XsG2qEVExxNIREEtIhI4BbWISOAU1CIigVNQi4gELukVXvYxs2fNbKaZzTCzkwpdWIp2fotIuUs6PO8vwCvufnF8NfI9C1gTAKbd3SIiQIKgNrP3A58HrgRw9zqgrrBlNdN4UhEpd0m6Pj4K1AIPmdkkM7s/vthtC2bW18yqzay6tra2ywsVESlXSYK6N3A8cLe7HwdsAfplTuTu/d29yt2rKioqurhMEZHylSSolwJL3X1cfPtZouAWkQJRj5+kyxnU7r4SWGJmR8R3nQ68W9CqRATQyckkknTUx7XA4/GIj/nAfxauJBERSZcoqN19MlBV4Fqyv3cp3lREJCDBHpmoYdQiIpFgg1pERCIKahGRwCmoRUQCp6AWEQmcglpEJHAKahGRwAUf1K7T54lImQs2qE0Hz0oZUwNF0gUb1CKCjvwSQEEtIhK8YINaDQkRkUiwQS0iIhEFtYhI4BTUIiKBS3Q+ajNbCGwCGoB6dy/auak1SklEyl3SK7wAnOruqwtWSQbtSxQRiajrQ0QkcEmD2oFXzWyCmfUtZEEiItJS0q6PU9x9mZntD7xmZjPdfXj6BHGA9wU45JBDurhMkfKiXTOSLlGL2t2Xxf/XAM8DJ2SZpr+7V7l7VUVFRddWKVKmtK9GIEFQm9l7zWzv1N/AmcC0QhcmIiKRJF0fHwKet+iY7t7AE+7+SkGrEhGRJjmD2t3nA8cUoZbs76/eOhEpc8EOz9NJmUREIsEGtYiIRIIN6tSh4zqEXETKXbBB/dDohQBMXLyutIWIiJRYsEHd0Bg1pZes3VbiSkRESivYoBYRkYiCWiRA2jcj6RTUIgHTMFUBBbWISPAU1CIigVNQi4gELvigVh+diJQ7BbWISOCCD2oRkXKnoBYRCVzwQa2B/yJS7hIHtZn1MrNJZvZSIQtq/b7FfDcRkfDk06L+ITCjUIW0xXR5TylL2pSUZomC2swOAs4D7i9sOSKSTs0UgeQt6juAnwGNbU1gZn3NrNrMqmtra7ukONA1E0VEcga1mZ0P1Lj7hPamc/f+7l7l7lUVFRVdVmA5+/WL0xkxp+tWeiLSPSVpUZ8MXGBmC4EBwGlm9lhBq0pTzn3UD49eyDceeLvUZYhIieUMane/3t0PcvdK4FLgTXe/ouCViYgI0A3GUYuIlLve+Uzs7sOAYQWpREREsgq+Ra0DXkSk3AUf1CIi5U5BLSISOAW1SIB0MjJJF3xQq4tayplpJ43QDYJaRKTcKahFRAKnoBYRCZyCWkQkcOEHtXamiEiZCz+oNU5JRMpc+EEtIlLmwg9qdX2ISJkLP6hFRMqcglokQNozI+kU1CIBU8efQLKL2+5uZm+b2TtmNt3MbipGYSIiEklyhZcdwGnuvtnMdgVGmtlgdx9b4NoAtShERHIGtbs7sDm+uWv8T11oIiJFkqiP2sx6mdlkoAZ4zd3HZZmmr5lVm1l1bW1tlxWo0XkiUu4SBbW7N7j7scBBwAlm1ifLNP3dvcrdqyoqKrq6ThGRspXXqA93Xw8MBc4uTDkiIpIpyaiPCjPbJ/57D+AMYGahCxMRkUiSUR8HAI+YWS+iYH/a3V8qbFkiIpKSZNTHFOC4ItQiIiJZBH9kogZ9SDnS2X0lXfhBrfF5Usa0+At0g6B2NS1EpMwFH9QiIuVOQS0iEjgFtYhI4BTUIiKBU1CLiAQu+KDW8DwRKXfhB3WpCxARKbHgg1pEpNwpqEVEAqegFgmQjsiVdApqkYCZ9tIICmoRkeAlucLLwWY21MzeNbPpZvbDYhTW/P7FfDcRkfAkucJLPfDf7j7RzPYGJpjZa+7+boFrExERErSo3X2Fu0+M/94EzAAOLHRhIiISyauP2swqiS7LNa4QxYhIS28vXFvqEiQAiYPazPYCngOuc/eNWR7va2bVZlZdW1vblTWKiJS1REFtZrsShfTj7j4w2zTu3t/dq9y9qqKioitrFBEpa0lGfRjwADDD3W8vfEkiIpIuSYv6ZOAbwGlmNjn+d26B62qiAf8iUu6SjPoY6e7m7ke7+7Hxv5eLUVy+hs6s4bAbXmbzjvpSlyLSKTqAXNL1qCMT//z6bHY2OPNqNpe6lE7TuR5EJKVHBbWISE8UflB3oItabVER6UnCD+o8aLejiPREPSqoU9S/KyI9Sc8K6h50qj2ta0QkJfig7jnRKyLSMcEHdUeoMSoiPUmPCmq1vkWkK/zrneVU9hvEhq07S10K0MOCOqUn9O/2gFkQ6bb6D58PwKK1W0pcSST4oM5n/2AP2pdYMIvXbKWy3yBenb6y1KVIO3pCY6M788CaSsEHdT60cOc2Zdl6AP45eXmJKxEJXygnhQs+qBW+IlLugg/qfKjrQ0S6QqqBGEqm9Kigbtb9m+E6ulKkdEL7+QUf1HntTCxcGSJSRgLL6USX4nrQzGrMbFoxCuoKoa0NQxTaXm2REHWnro+HgbMLXEeb8tnraqF8qgELZS+2SMhC63pMcimu4cDaItQiIiX23UerueaxCaUuIxihNGx6d9ULmVlfoC/AIYcc0lUv26FNj7DWhR3T3eZh2rIN7LlbLw6t2KvUpUgnDJm+qtQlBKHHjvpw9/7uXuXuVRUVFV31snn1N+f6TBsanXHz13SqHsnu/L+N5LT/favUZYh0qSRBvXTd1oLXEfyoj45oK9zveWseX+s/ltFzVxe3oICkFrzAuuAkg3b2llY+n//OhsJ/Vz0qqHOt/Wav2gRAzaYdRagmTKmPSEEtkluSPurGIvyYkgzPexIYAxxhZkvN7KqCV9Xi/ZNPm+vzagys30l6tmXrt5W6BOmgfLK3GI2eJKM+LnP3A9x9V3c/yN0fKHxZzboyVFNrvl0CSupBU1ZQ2W8QKzdsb3G/Wrxda8aKjUxavK7FfWPmreE3/3q3IO83fHYtJ//xTV6euiKv501cvI6Rc8q3a64zjv71EP7riYld8lqpn1+yqAigRd2d5PpQPcCgHjB+MQAzV24scSU92zl/GcH/u2t0i/suu28sD45aUJD3m748+j7fWbI+r+d95a7RXPHAuEKU1ONt3F7PS1PyWzHmkiQpGkNoUXdHbQ1Wb2yM/t8lnJxuEtLKQzqvaadtacuQDsrngJcguj66k1wd/6muj5COYGxoLE0rv7292tt3NrBoTRhXtuiumnfaKqq7syQ/y2KM0Ak+qN2hsdGZsKjzB0emNlFCalGnVh4rN27PMWXXSLLg/eipyXzh1mFs39lQ+IJyWLVxO31uHMKMFV3TNXTG7W/x+ruFP6gjoLaAdEBz9Ob+ItWijj0wcgEX3T2GEXNqE03f1ucWYh91auXxk2feaXF/KcfRDp1VA3T9sKNhs2qo7DeImjxWSm/OrGHzjnoeGb2QZeu38eTbiztVw5yazfQbOLVTr5GP7tCgbmz0Drf8F6zewtyazV1cUQDSRojVNzS2+/kEMTwvBKnxzyvW5/iB58jf5q6Prqiq2dG/HsL3H+/g3uZAfsgzV25kYjwqItWX36uLNz0eGxuF7OQ8d7ClXHH/OK4fOJWN2zt3ZejM2WrswN6g+oZGDr9hME+PX9LGe0RvUqyv173jYfuJX73CGX8e3qHnnnrbML50e/c+InX2qk2tPrvULXfn4zcM5neDZrT5fLWoiUI138+hrQ8udXdXt6g3bq9nUJ7DsFIaitzkmrAoCuN1W6Owq9m0nUVrtnD2HSP4SjwqIlVTry7+nFIB2dG95Gs2RwcqeWPn6sicrfXbdnLLKzP59G9fS/wa23Y2UNfQyG9eajm8z93Z2dBc4Oh5a9i8o75T9SbR77mpfPT6lzv03Lr6xp7ZKk5g1NzVnPnn4Tz5dvYV7pxV0efyjzELi1dUFuEHdXozOUduLFsXHWDQVrdBiAe8FGOzKd19I6LhaBPjwD7h92/whVuHtZimMzs4V29u+6jPplZmB+d54/Yo8JZv6NyBJNnm6+5h81izpS7xa6S2NuobW641Hhu7iMNuGNx09OuMFRu5tiNje/P8iJ6qzh40nVGzaTuL12xl1spNbKsr/f6KQphfGwXxuys2tLg/tYxeE28pt7fIlmXXx+h5q1u1QJrOZJXjuakjwWau2JT18ZD7qFvc1+hNa/L2uDv9h89j/dbkAdP03ATTdORj+uq9Y9p8LBVu+WxFpE54k/6USYs71nWSkjlb6bdz9UempOYlI6d5Ib66e/qJemauzL48hu6E37/B528dyll3DOf7XXQgSVJvzFjFgyM7NsZ98pL1LFqzhU/dOITpyzfkfkIWmUtAe0tE2XV9nH3HcC6/bxzXDZjc4v5UCznpsLq93pP97K0hHpmYLRT6j5jP+X8b2XS7rZNIjV+4jj+8PJOfPzcl7/dN8gkk/byHxTsfAebXtj2sb5c43B4atZCaTa33N0xduoHnJy1tcd+dQ+e1fp0sZW3YtpPxC5ONDMqcr/SbH79hMI+Na3+H5ZYd9U1bejvTkvqt2bUsWB3Nf69dmn9aqa945Ybt3DZkVqIVQbG7xCA6mrItYwtw1slXp6/knrdaf78AVz1S3apbKakv3zmKL9w6jE3xTuiukFpEZq3c1OogpmJ8U0EFdarl0eoovfiTuOP12e0+/9iD9wHgoA/ukfXxXAe8TFu2IdmPqNG55rEJTTvf2nNp/zH89Y05bT6ebbMpc0G4/P5x/HDAJL73eMsTuqf6Qjdtb78P9PqBU1qNtuiqddXcms1c+dD4Fvet3LCdW16Z2WonXepzn7BoXdadr//x95H86Kl3Wt2fKVvt//+R8Vxyz5imIYUNjc79I+ZnHWK4S8ZSnzn+fuDEliuLdO7OJ28cQr+BU+LbzY9968G3WRt3n/ROW8jWxVs83354PH8fOpeJGVsEdfWNLFi9hWnLmlt/D41a2GYNEDVqKvsNaneafH3zwbfbfKwQm/d9H53AHwfPbLo9v3Yzlf0GtfgcsnF3NmxLtkM5V6Os6VDxjGUgc3ZTN8+6YzgX3jmqxWNTc9TbFYIK6pSl65r7IEfOXd30IaXfn83uu0azs2Fr9i9xTNwquG/E/FahP3ruas7/28g2fyCzV23iugGTqG9oZPXmHQyetpKrH20ZnOuy9HGOnb+W219rvYJZvGYr9741r8Wm8yX3jG6xIyrdPycv5+WpK5tuvzW7tt2dVE+NX8zl940FaNpRcnXalTt2NjhfvHVom8+HaIWZ6/zdW+ta1/CDJydx97B5TMlYgNP7r7fsyK/Pc9Ha5pZ6Zot41cbtTFsWrdxTgTJw4lJ+N2hG1pVkqx9vxs36Nk5buX1nQ1NX1cCJy9qtN/09dtRH3+m78VjwhowV2I0vTufU24a12IrKdY7jznSn1NU3sn5rHd99tLppB20u2XI66XmYV2/e0TRyKyXbCvSO16PvKv1zmLBoHVOWtlyx3fLKLI656dVW58fJJnNZGfD2Ylak7eNo6wIBmSumzO8s3YoinHwryKDOlN7KXb+1rmkT7ftPTOSUW95semzs/GjT95osrbV/Tm7+YQ2dVdu0UGzf2cDaLXUsWrs1fqym1XMBfjhgMi9MXs6sVZua+1ozvrzjcowaqKtv5Au3DmXorBqueGAcNw+e2fTjhagrI9fKaODEpUxbtoFvPfg2P3k6an2OnremKVCfHr+Eyn6D+PlzUxk9r2XIZrbkFq5p/4d29h0j+Fr/sW2uPCB7i2Vb/CM0or3qlf0G8fLUFYya21zPrr2Myn6DOOcvI7Juxcyr3cyX01ouqe829bopz09aymf/8EbTe6bqSd2+a1jrTetFa7by6xenN79exizsbGhk0/adVPYbxOPjFgGwfP02PvHLV3goy7lBbh48o1Xrtr2RjZkhMHJu6y6H9HMcz62JWppPj1/C3JpNLVqTv3xhGvVp38/6rXWc/Mc3WR6Hx+rNO1p9vof/YjDH/uY1hkxf1fQ7yJS5Vdfozl/fmMOStc3LzCm3tL+iT/n8n4ZyZsbwv2wHMFVn6bq66O7RXPD3UVT2G8ToeVEXYKq7ZOXGaMRSe1vBveKE21pXz6btO+k3cCqX3zeO8QvXcsfrs7kxbTmAaGfwzYNn5PwdpttahB2tXXYprkJK7aABOPY3URi+dO0pDIpPwHLG7W/xj6tOaPc1fpjR753ymd+9zqYd9fzuy30AGDFnNfNrN3PgB/bgPb17MbdmMz9/bkrTwrB2Sx3feaQagDVb6hiTEYYNjU6vXYx1W+paHG0YdatEIfE/A6e2OcJg4MSlDJ62MutjAD9++h0Ojrt2NqW1qL/WfywjfnYqN/1reltPTeS+4fNb3XfYDYP59skf5Vf/cRQAx9z0Khu27WTeH87NOtY6tSk4Yk4t4xZEP77vZaw8d41/QTNWbOTh0Qs565P/1uLx21+b3eZ468076vnx05O59eJjeGNG6xXrM9VLmka1tOXhtL7LzDmYuXITq+Lv7obnpzFh4TpOP/JDQPRDznTvW60/s2cmtOw+ueSe5hNCPT9xGSceui+rN+9g/dadLFnbOhTSV44PjIxe/2dZ9kU8OnYRj6bV9Pc357Js/TY+98fmBsyxB+/D5CXr+cV5R7LHbr1aPT+bzM37nQ3O7a/N5vbXZrPwj+dlfU59QyNrt9ax/967N9136PWDmrZC1m6p4wN77spdw+Zx4D7N3ZPPT1rKwInLWJ6jhXz5feNavPe4+Wu4efBMTvvE/tx68dFZn/PY2MVN4/d/dX60/C5YvYVL7mm50zu1tfeLF6a1+f7XDZjU9Hd6I+3h0Qu54bwjueL+cfz0rCOoqvxgu/PREVaIcxFUVVV5dXV13s/ryj635645iX7PTWVOzWYuOv4gnsvS73jSofs2dYdkM+Jnp/Lvf0rWaki554rjObvPAV3ef9hRPznzcG57tf2+/aQm/vIMRsypbVrp/emio/nY/ntx0d2jczwzPz847eO8NHVFuzsmAfZ9726JhtT958mV7fb5TrvpLPrcOKTFfbmWjc669eKj+emz+e8EDsFL155CnwPf32IZn/27czj8F4NbTHfZCQe3OT65o6bfdBafzPiuusqofqdxctoKrj279d6FuvrmleldXz++qTHS1oosFzOb4O5VWR/rqUEtIlIKhQjqRH3UZna2mc0ys7lm1q9DVYiISIckuRRXL+BO4BzgKOAyMzuq0IWJiEgkSYv6BGCuu8939zpgAHBhYcsSEZGUJEF9IJC+R2BpfF8LZtbXzKrNrLq2NtnpSEVEepqOnI0xly4bnufu/YH+EO1M7MhrdLQTXkSkJ0vSol4GHJx2+6D4PhERKYIkQT0eOMzMPmpmuwGXAi8WtiwREUnJ2fXh7vVm9l/AEKAX8KC7d+7wNxERSSxRH7W7vwx07PIRIiLSKd3ipEwiIuVMQS0iEjgFtYhI4BTUIiKBK8jZ88ysFsh+otvc9gOyXySwe+ju9UP3n4fuXj90/3no7vVD8efhI+5eke2BggR1Z5hZdVun+usOunv90P3nobvXD91/Hrp7/RDWPKjrQ0QkcApqEZHAhRjU/UtdQCd19/qh+89Dd68fuv88dPf6IaB5CK6PWkREWgqxRS0iImkU1CIigStZUOe6YK6ZvcfMnoofH2dmlcWvsm0J6v+xmb1rZlPM7A0z+0gp6mxL0gsWm9lFZuZmFsQwpXRJ5sHMvhp/D9PN7Ili19ieBMvQIWY21MwmxcvRuaWosy1m9qCZ1ZjZtDYeNzP7azx/U8zs+GLXmEuCefh6XPtUMxttZscUu0YA3L3o/4hOlzoPOBTYDXgHOCpjmu8B98R/Xwo8VYpaO1H/qcCe8d/XdLf64+n2BoYDY4GqUtfdge/gMGAS8IH49v6lrjvP+vsD18R/HwUsLHXdGfV9HjgemNbG4+cCgwEDTgTGlbrmDszD59KWn3NKNQ+lalEnuWDuhcAj8d/PAqebmRWxxvbkrN/dh7r71vjmWKIr44Qi6QWLfwvcAmwvZnEJJZmH7wB3uvs6AHevKXKN7UlSvwPvi/9+P7C8iPXl5O7DgbXtTHIh8A+PjAX2MbMDilNdMrnmwd1Hp5YfSvg7LlVQJ7lgbtM07l4PbAD2LUp1uSW64G+aq4haFqHIWX+8mXqwuw8qZmF5SPIdHA4cbmajzGysmZ1dtOpyS1L/r4ErzGwp0fngry1OaV0m31qHq5wAAALMSURBVN9J6Er2O+6yi9tKdmZ2BVAFfKHUtSRlZrsAtwNXlriUzupN1P3xRaKW0HAz+5S7ry9pVcldBjzs7v9rZicBj5pZH3dvLHVh5cbMTiUK6lNK8f6lalEnuWBu0zRm1pto029NUarLLdEFf83sS8ANwAXuvqNItSWRq/69gT7AMDNbSNS/+GJgOxSTfAdLgRfdfae7LwBmEwV3CJLUfxXwNIC7jwF2JzpRUHfRIy6MbWZHA/cDF7p7STKoVEGd5IK5LwLfiv++GHjT4x79AOSs38yOA+4lCumQ+kYhR/3uvsHd93P3SnevJOqbu8Ddq0tTblZJlqEXiFrTmNl+RF0h84tZZDuS1L8YOB3AzI4kCuraolbZOS8C34xHf5wIbHD3FaUuKh9mdggwEPiGu88uWSEl3Nt6LlELZx5wQ3zfb4gCAaKF8hlgLvA2cGipau1g/a8Dq4DJ8b8XS11zPvVnTDuMwEZ9JPwOjKgL511gKnBpqWvOs/6jgFFEI0ImA2eWuuaM+p8EVgA7ibZergKuBq5O+/zvjOdvaqDLUK55uB9Yl/Y7ri5FnTqEXEQkcDoyUUQkcApqEZHAKahFRAKnoBYRCZyCWkSkk3Kd3CnL9HmdLEyjPkREOsnMPg9sJjq3SZ8c0x5GdCDTae6+zsz29xzHWqhFLSLSSZ7l5E5m9jEze8XMJpjZCDP7RPxQ3icLU1CLiBRGf+Bad/808BPgrvj+vE8WppMyiYh0MTPbi+hc1s+knZ35PfH/eZ8sTEEtItL1dgHWu/uxWR5bSnQBgp3AAjNLnSxsfHsvJiIiXcjdNxKF8CXQdFmy1GW88j5ZmIJaRKSTzOxJYAxwhJktNbOrgK8DV5nZO8B0mq/gMwRYY2bvAkOBn3qO06dqeJ6ISODUohYRCZyCWkQkcApqEZHAKahFRAKnoBYRCZyCWkQkcApqEZHA/R9nYMQlsycq7gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBptphxsDdOp"
      },
      "source": [
        "# TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        },
        "id": "zBNqI5tzjXA0",
        "outputId": "a77a5caf-79e0-4f87-9bdb-775774e115ef"
      },
      "source": [
        "## TESTING AGENT\n",
        "# make new environment with Test data\n",
        "test_env = StockEnv(test_data,initial_investment)\n",
        "\n",
        "# set epsilon to very small value, so that very little exploration is needed\n",
        "agent.epsilon = .01\n",
        "\n",
        "# record final value of portfolio (at end of episode)\n",
        "testing_portfolio_value = []\n",
        "\n",
        "# test the agent multiple times\n",
        "for e in range(n_episodes):\n",
        "  t0 = datetime.now() #start time\n",
        "  \n",
        "  # get portfolio value after playing 1 episode\n",
        "  val = play_one_episode(agent, test_env, scaler, 'test')\n",
        "\n",
        "  dt = datetime.now() - t0 #duration elapsed from start time\n",
        "  \n",
        "  if (e+1) % 100 == 0:\n",
        "    print('episode %d of %d, portfolio value: %.2f, duration: %.2f' % (e+1,n_episodes,val,dt.seconds+dt.microseconds*1e-6))\n",
        "  testing_portfolio_value.append(val)\n",
        "\n",
        "# plot\n",
        "_ = plt.hist(testing_portfolio_value, bins = 50)\n",
        "_ = plt.title('TEST')\n",
        "\n",
        "# print values\n",
        "print('\\n')\n",
        "print('mean: %.2f' % np.mean(testing_portfolio_value))\n",
        "print('min: %d' % np.min(testing_portfolio_value))\n",
        "print('max: %d' % np.max(testing_portfolio_value))\n",
        "print('\\n')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode 100 of 2000, portfolio value: 24038.84, duration: 0.07\n",
            "episode 200 of 2000, portfolio value: 23876.13, duration: 0.12\n",
            "episode 300 of 2000, portfolio value: 25074.49, duration: 0.08\n",
            "episode 400 of 2000, portfolio value: 22872.53, duration: 0.11\n",
            "episode 500 of 2000, portfolio value: 25728.20, duration: 0.12\n",
            "episode 600 of 2000, portfolio value: 25267.67, duration: 0.08\n",
            "episode 700 of 2000, portfolio value: 23249.60, duration: 0.07\n",
            "episode 800 of 2000, portfolio value: 22954.49, duration: 0.09\n",
            "episode 900 of 2000, portfolio value: 23026.34, duration: 0.12\n",
            "episode 1000 of 2000, portfolio value: 22999.59, duration: 0.06\n",
            "episode 1100 of 2000, portfolio value: 23157.65, duration: 0.07\n",
            "episode 1200 of 2000, portfolio value: 24867.17, duration: 0.10\n",
            "episode 1300 of 2000, portfolio value: 23847.24, duration: 0.08\n",
            "episode 1400 of 2000, portfolio value: 22986.96, duration: 0.12\n",
            "episode 1500 of 2000, portfolio value: 23090.89, duration: 0.12\n",
            "episode 1600 of 2000, portfolio value: 24668.97, duration: 0.12\n",
            "episode 1700 of 2000, portfolio value: 25301.57, duration: 0.10\n",
            "episode 1800 of 2000, portfolio value: 24650.86, duration: 0.09\n",
            "episode 1900 of 2000, portfolio value: 23253.38, duration: 0.12\n",
            "episode 2000 of 2000, portfolio value: 23387.58, duration: 0.11\n",
            "\n",
            "\n",
            "mean: 23860.48\n",
            "min: 16203\n",
            "max: 27973\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2-QG16wJcf7"
      },
      "source": [
        "It seems, on average, this RL agent gets investment returns of ~19%, running a profit majority of the time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RqjOdyUvPOu"
      },
      "source": [
        "# Final Notes: Potential improvements.\n",
        "\n",
        "1. Incorporate metadata, e.g. sentiment of the stock (Twitter, Google etc.)\n",
        "2. Incorporate price changes\n",
        "3. Use returns instead of prices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QMS9OAGiuhz"
      },
      "source": [
        "# MISC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw_81jYSDeAk"
      },
      "source": [
        "# # set up environment for test mode\n",
        "# #if args.mode == 'test':\n",
        "# if MODE == 'test':\n",
        "#   # load StandardScaler from training dataset\n",
        "#   with open(f'{models_folder}/scaler.pkl','rb' as f):\n",
        "#     scaler = pickle.load(f)\n",
        "\n",
        "#   # make new environment with Test data\n",
        "#   env = StockEnv(test_data,initial_investment)\n",
        "\n",
        "#   # set epsilon to not 1, so that no exploration is needed\n",
        "#   agent.epsilon = .01\n",
        "\n",
        "#   # load trained weights\n",
        "#   scaler = agent.load(f'{models_folder}/linear.npz')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r6uCY7OiuSO"
      },
      "source": [
        "# # add argument parser so we can run script in CLI (outside of .ipynb environment)\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument('-m','--mode',type = str, required = True, help = 'either \"train\" or \"test\"')\n",
        "# args = parser.parse_args()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EE6I0Dv3ix0l"
      },
      "source": [
        "# model_folder = './'\n",
        "# rewards_folder = './'\n",
        "\n",
        "# # make directories if non-existent\n",
        "# make_dir(model_folder)\n",
        "# make_dir(rewards_folder)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNS0Pi3wL94_"
      },
      "source": [
        "# credits\n",
        "\n",
        "https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python"
      ]
    }
  ]
}